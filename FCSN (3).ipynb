{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCSN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KtSrNlv1PCx",
        "outputId": "822de9e2-35d8-4e0d-9bc2-0493b4db8f02"
      },
      "source": [
        "!pip install tensorboardX \n",
        "!pip install torchsummary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po6J8vX-2Cj-"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "import h5py\n",
        "from prettytable import PrettyTable\n",
        "from torchsummary import summary\n",
        "import torch.nn as nn\n",
        "from collections import OrderedDict \n",
        "\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "H5PY_DEFAULT_READONLY=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0W3A3BT2IDg"
      },
      "source": [
        "class FCSN(nn.Module):\n",
        "    def __init__(self, n_class=2):\n",
        "        super(FCSN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(OrderedDict([\n",
        "            ('convRED_1', nn.Conv1d(1024, 512, 3, padding=1)),\n",
        "            ('convRED_2', nn.Conv1d(512, 512, 3, padding=1)),\n",
        "            ('convRED_3', nn.Conv1d(512,256 , 3, padding=1)),\n",
        "            ('conv1_1', nn.Conv1d(256, 256, 3, padding=1)),\n",
        "            ('bn1_1', nn.BatchNorm1d(256)),\n",
        "            ('relu1_1', nn.ReLU(inplace=True)),\n",
        "            ('conv1_2', nn.Conv1d(256, 256, 3, padding=1)),\n",
        "            ('bn1_2', nn.BatchNorm1d(256)),\n",
        "            ('relu1_2', nn.ReLU(inplace=True)),\n",
        "            ('pool1', nn.MaxPool1d(2, stride=2, ceil_mode=True))\n",
        "            ])) # 1/2\n",
        "\n",
        "        self.conv2 = nn.Sequential(OrderedDict([\n",
        "            ('conv2_1', nn.Conv1d(256, 256, 3, padding=1)),\n",
        "            ('bn2_1', nn.BatchNorm1d(256)),\n",
        "            ('relu2_1', nn.ReLU(inplace=True)),\n",
        "            ('pool2', nn.MaxPool1d(2, stride=2, ceil_mode=True))\n",
        "            ])) # 1/4\n",
        "\n",
        "        self.conv3 = nn.Sequential(OrderedDict([\n",
        "            ('conv3_1', nn.Conv1d(256, 256, 3, padding=1)),\n",
        "            ('bn3_1', nn.BatchNorm1d(256)),\n",
        "            ('relu3_1', nn.ReLU(inplace=True)),\n",
        "            ('conv3_2', nn.Conv1d(256, 256, 3, padding=1)),\n",
        "            ('bn3_2', nn.BatchNorm1d(256)),\n",
        "            ('relu3_2', nn.ReLU(inplace=True)),\n",
        "            ('pool3', nn.MaxPool1d(2, stride=2, ceil_mode=True))\n",
        "            ])) # 1/8\n",
        "\n",
        "        self.conv4 = nn.Sequential(OrderedDict([\n",
        "            ('conv4_1', nn.Conv1d(256, 512, 3, padding=1)),\n",
        "            ('bn4_1', nn.BatchNorm1d(512)),\n",
        "            ('relu4_1', nn.ReLU(inplace=True)),\n",
        "            ('conv4_2', nn.Conv1d(512, 512, 3, padding=1)),\n",
        "            ('bn4_2', nn.BatchNorm1d(512)),\n",
        "            ('relu4_2', nn.ReLU(inplace=True)),\n",
        "            ('pool4', nn.MaxPool1d(2, stride=2, ceil_mode=True))\n",
        "            ])) # 1/16\n",
        "\n",
        "        self.conv5 = nn.Sequential(OrderedDict([\n",
        "            ('conv5_1', nn.Conv1d(512, 512, 3, padding=1)),\n",
        "            ('bn5_1', nn.BatchNorm1d(512)),\n",
        "            ('relu5_1', nn.ReLU(inplace=True)),\n",
        "            ('conv5_2', nn.Conv1d(512, 512, 3, padding=1)),\n",
        "            ('bn5_2', nn.BatchNorm1d(512)),\n",
        "            ('relu5_2', nn.ReLU(inplace=True)),\n",
        "            ('pool5', nn.MaxPool1d(2, stride=2, ceil_mode=True))\n",
        "            ])) # 1/32\n",
        "\n",
        "        self.conv6 = nn.Sequential(OrderedDict([\n",
        "            ('fc6', nn.Conv1d(512, 1024, 1)),\n",
        "            ('bn6', nn.BatchNorm1d(1024)),\n",
        "            ('relu6', nn.ReLU(inplace=True)),\n",
        "            ('drop6', nn.Dropout())\n",
        "            ]))\n",
        "   \n",
        "        self.conv7 = nn.Sequential(OrderedDict([\n",
        "            ('fc7', nn.Conv1d(1024, 1024, 1)),\n",
        "            ('bn7', nn.BatchNorm1d(1024)),\n",
        "            ('relu7', nn.ReLU(inplace=True)),\n",
        "            ('drop7', nn.Dropout())\n",
        "            ]))\n",
        "\n",
        "        self.conv8 = nn.Sequential(OrderedDict([\n",
        "            ('fc8', nn.Conv1d(1024, n_class, 1)),\n",
        "            ('bn8', nn.BatchNorm1d(n_class)),\n",
        "            ('relu8', nn.ReLU(inplace=True)),\n",
        "            ]))\n",
        "\n",
        "        self.conv_pool4 = nn.Conv1d(512, n_class, 1)\n",
        "        self.bn_pool4 = nn.BatchNorm1d(n_class)\n",
        "\n",
        "        self.conv_pool3=nn.Conv1d(256,n_class,1)\n",
        "        self.bn_pool3=nn.BatchNorm1d(n_class)\n",
        "\n",
        "        self.deconv1 = nn.ConvTranspose1d(n_class, n_class, 4, padding=1, stride=2, bias=False)\n",
        "        self.deconv2 = nn.ConvTranspose1d(n_class, n_class, 4, padding=1, stride=2, bias=False)\n",
        "        self.deconv3 = nn.ConvTranspose1d(n_class, n_class, 8, stride=8, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h = x\n",
        "        h = self.conv1(h)\n",
        "        h = self.conv2(h)\n",
        "        h = self.conv3(h)\n",
        "        pool3 = h\n",
        "        h = self.conv4(h)\n",
        "        pool4 = h\n",
        "\n",
        "        h = self.conv5(h)\n",
        "        h = self.conv6(h)\n",
        "        h = self.conv7(h)\n",
        "        h = self.conv8(h)\n",
        "\n",
        "        h = self.deconv1(h)\n",
        "        upscore2 = h\n",
        "\n",
        "        h = self.conv_pool4(pool4)\n",
        "        h = self.bn_pool4(h)\n",
        "        score_pool4 = h\n",
        "\n",
        "        h = upscore2 + score_pool4\n",
        "\n",
        "        h = self.deconv2(h)\n",
        "\n",
        "        upscore3 = h\n",
        "        h = self.conv_pool3(pool3)\n",
        "        h = self.bn_pool3(h)\n",
        "        score_pool3 = h\n",
        "\n",
        "        h = upscore3 + score_pool3\n",
        "        \n",
        "        h= self.deconv3(h)\n",
        "        \n",
        "        return h\n",
        "\n",
        "\n",
        "#import torch\n",
        "#net = FCSN()\n",
        "#data = torch.randn((1, 1024, 320))\n",
        "#print(net(data).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhmiahMD22nJ"
      },
      "source": [
        "def knapsack(v, w, max_weight):\n",
        "    rows = len(v) + 1\n",
        "    cols = max_weight + 1\n",
        "\n",
        "    # adding dummy values as later on we consider these values as indexed from 1 for convinence\n",
        "    \n",
        "    v = np.r_[[0], v]\n",
        "    w = np.r_[[0], w]\n",
        "\n",
        "    # row : values , #col : weights\n",
        "    dp_array = [[0 for i in range(cols)] for j in range(rows)]\n",
        "\n",
        "    # 0th row and 0th column have value 0\n",
        "\n",
        "    # values\n",
        "    for i in range(1, rows):\n",
        "        # weights\n",
        "        for j in range(1, cols):\n",
        "            # if this weight exceeds max_weight at that point\n",
        "            if j - w[i] < 0:\n",
        "                dp_array[i][j] = dp_array[i - 1][j]\n",
        "\n",
        "            # max of -> last ele taken | this ele taken + max of previous values possible\n",
        "            else:\n",
        "                dp_array[i][j] = max(dp_array[i - 1][j], v[i] + dp_array[i - 1][j - w[i]])\n",
        "\n",
        "    # return dp_array[rows][cols]  : will have the max value possible for given wieghts\n",
        "\n",
        "    chosen = []\n",
        "    i = rows - 1\n",
        "    j = cols - 1\n",
        "\n",
        "    # Get the items to be picked\n",
        "    while i > 0 and j > 0:\n",
        "\n",
        "        # ith element is added\n",
        "        if dp_array[i][j] != dp_array[i - 1][j]:\n",
        "            # add the value\n",
        "            chosen.append(i-1)\n",
        "            # decrease the weight possible (j)\n",
        "            j = j - w[i]\n",
        "            # go to previous row\n",
        "            i = i - 1\n",
        "\n",
        "        else:\n",
        "            i = i - 1\n",
        "\n",
        "    return dp_array[rows - 1][cols - 1], chosen\n",
        "\n",
        "\n",
        "# values = list(map(int, input().split()))\n",
        "# weights = list(map(int, input().split()))\n",
        "# max_weight = int(input())\n",
        "\n",
        "# max_value, chosen = knapsack(values, weights, max_weight)\n",
        "\n",
        "# print(\"The max value possible is\")\n",
        "# print(max_value)\n",
        "\n",
        "# print(\"The index chosen for these are\")\n",
        "# print(' '.join(str(x) for x in chosen))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deoiBWtx3DVh"
      },
      "source": [
        "def eval_metrics(y_pred, y_true):\n",
        "    overlap = np.sum(y_pred * y_true)\n",
        "    precision = overlap / (np.sum(y_pred) + 1e-8)\n",
        "    recall = overlap / (np.sum(y_true) + 1e-8)\n",
        "    if precision == 0 and recall == 0:\n",
        "        fscore = 0\n",
        "    else:\n",
        "        fscore = 2 * precision * recall / (precision + recall)\n",
        "    return [precision, recall, fscore]\n",
        "\n",
        "\n",
        "def select_keyshots(video_info, pred_score):\n",
        "    N = video_info['length'][()]\n",
        "    cps = video_info['change_points'][()]\n",
        "    weight = video_info['n_frame_per_seg'][()]\n",
        "    pred_score = np.array(pred_score.cpu().data)\n",
        "    pred_score = upsample(pred_score, N)\n",
        "    pred_value = np.array([pred_score[cp[0]:cp[1]].mean() for cp in cps])\n",
        "    _, selected = knapsack(pred_value, weight, int(0.15 * N))\n",
        "    selected = selected[::-1]\n",
        "    key_labels = np.zeros(shape=(N, ))\n",
        "    for i in selected:\n",
        "        key_labels[cps[i][0]:cps[i][1]] = 1\n",
        "    return pred_score.tolist(), selected, key_labels.tolist()\n",
        "\n",
        "\n",
        "def upsample(down_arr, N):\n",
        "    up_arr = np.zeros(N)\n",
        "    ratio = N // 320\n",
        "    l = (N - ratio * 320) // 2\n",
        "    i = 0\n",
        "    while i < 320:\n",
        "        up_arr[l:l+ratio] = np.ones(ratio, dtype=int) * down_arr[i]\n",
        "        l += ratio\n",
        "        i += 1\n",
        "    return up_arr\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO_-AsDW3c2f"
      },
      "source": [
        "\n",
        "class Config():\n",
        "    \"\"\"Config class\"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "\n",
        "        # Path\n",
        "        self.data_path = '/content/sample_data/data/fcsn_tvsum.h5'\n",
        "        self.save_dir = 'save_dir'\n",
        "        self.score_dir = 'score_dir'\n",
        "        self.log_dir = 'log_dir'\n",
        "\n",
        "        # Model\n",
        "        self.mode = 'train'\n",
        "        self.gpu = True\n",
        "        self.n_epochs = 200\n",
        "        self.n_class = 2\n",
        "        self.lr = 1e-5\n",
        "        self.momentum = 0.9\n",
        "        self.batch_size = 5\n",
        "\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "    def __repr__(self):\n",
        "        config_str = 'Configurations\\n' + pprint.pformat(self.__dict__)\n",
        "        return config_str\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xvw4-dH3t-E"
      },
      "source": [
        "class VideoData(object):\n",
        "    \"\"\"Dataset class\"\"\"\n",
        "    def __init__(self, data_path):\n",
        "        self.data_file = h5py.File(data_path, \"r\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_file)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        index += 1\n",
        "        video = self.data_file['video_'+str(index)]\n",
        "        feature = torch.tensor(video['feature'][()]).t()\n",
        "        label = torch.tensor(video['label'][()], dtype=torch.long)\n",
        "        return feature, label, index\n",
        "    \n",
        "\n",
        "def get_loader(path, batch_size=5):\n",
        "    dataset = VideoData(path)\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [len(dataset) - len(dataset) // 5, len(dataset) // 5])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "    return train_loader, test_dataset\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     loader = get_loader('fcsn_dataset.h5')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTLNlbIx3Mvl"
      },
      "source": [
        "class Solver(object):\n",
        "    \"\"\"Class that Builds, Trains FCSN model\"\"\"\n",
        "\n",
        "    def __init__(self, config=None, train_loader=None, test_dataset=None):\n",
        "        self.config = config\n",
        "        self.train_loader = train_loader\n",
        "        self.test_dataset = test_dataset\n",
        "        self.losses=[]\n",
        "        # model\n",
        "        self.model = FCSN(self.config.n_class)\n",
        "        summary(self.model,(1024,320))\n",
        "\n",
        "        # optimizer\n",
        "        if self.config.mode == 'train':\n",
        "            self.optimizer = optim.Adam(self.model.parameters())\n",
        "            # self.optimizer = optim.SGD(self.model.parameters(), lr=config.lr, momentum=self.config.momentum)\n",
        "            self.model.train()\n",
        "\n",
        "        if self.config.gpu:\n",
        "            pass\n",
        "            #self.model = self.model.cuda()\n",
        "\n",
        "        if not os.path.exists(self.config.score_dir):\n",
        "            os.mkdir(self.config.score_dir)\n",
        "\n",
        "        if not os.path.exists(self.config.save_dir):\n",
        "            os.mkdir(self.config.save_dir)\n",
        "\n",
        "        if not os.path.exists(self.config.log_dir):\n",
        "            os.mkdir(self.config.log_dir)\n",
        "\n",
        "    @staticmethod\n",
        "    def sum_loss(pred_score, gt_labels, weight=None):\n",
        "        n_batch, n_class, n_frame = pred_score.shape\n",
        "        log_p = torch.log_softmax(pred_score, dim=1).reshape(-1, n_class)\n",
        "        gt_labels = gt_labels.reshape(-1)\n",
        "        criterion = torch.nn.NLLLoss(weight)\n",
        "        loss = criterion(log_p, gt_labels)\n",
        "        losses.append(loss.item())\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        writer = SummaryWriter(log_dir=self.config.log_dir)\n",
        "        t = trange(self.config.n_epochs, desc='Epoch', ncols=80)\n",
        "        for epoch_i in t:\n",
        "            sum_loss_history = []\n",
        "\n",
        "            for batch_i, (feature, label,  _) in enumerate(tqdm(self.train_loader, desc='Batch', ncols=80, leave=False)):\n",
        "\n",
        "                # [batch_size, 1024, seq_len]\n",
        "                feature.requires_grad_()\n",
        "                # => cuda\n",
        "                if self.config.gpu:\n",
        "                    pass\n",
        "                    #feature = feature.cuda()\n",
        "                    #label = label.cuda()\n",
        "\n",
        "                # ---- Train ---- #\n",
        "                pred_score = self.model(feature)\n",
        "\n",
        "                label_1 = label.sum() / label.shape[0]\n",
        "                label_0 = label.shape[1] - label_1\n",
        "                weight = torch.tensor([label_1, label_0], dtype=torch.float)\n",
        "\n",
        "                if self.config.gpu:\n",
        "                    pass\n",
        "                    #weight = weight.cuda()\n",
        "\n",
        "                loss = self.sum_loss(pred_score, label, weight)\n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "                self.optimizer.zero_grad()\n",
        "                sum_loss_history.append(loss)\n",
        "\n",
        "            mean_loss = torch.stack(sum_loss_history).mean().item()\n",
        "            t.set_postfix(loss=mean_loss)\n",
        "            writer.add_scalar('Loss', mean_loss, epoch_i)\n",
        "\n",
        "            if (epoch_i+1) % 20 == 0:\n",
        "                ckpt_path = self.config.save_dir + '/epoch-{}.pkl'.format(epoch_i)\n",
        "                tqdm.write('Save parameters at {}'.format(ckpt_path))\n",
        "                torch.save(self.model.state_dict(), ckpt_path)\n",
        "                self.evaluate(epoch_i)\n",
        "                self.model.train()\n",
        "\n",
        "    def evaluate(self, epoch_i):\n",
        "        self.model.eval()\n",
        "        out_dict = {}\n",
        "        eval_arr = []\n",
        "        table = PrettyTable()\n",
        "        table.title = 'Eval result of epoch {}'.format(epoch_i)\n",
        "        table.field_names = ['ID', 'Precision', 'Recall', 'F-score']\n",
        "        table.float_format = '1.3'\n",
        "\n",
        "        with h5py.File(self.config.data_path, \"r\") as data_file:\n",
        "            for feature, label, idx in tqdm(self.test_dataset, desc='Evaluate', ncols=1, leave=False):\n",
        "                if self.config.gpu:\n",
        "                    pass\n",
        "                    #feature = feature.cuda()\n",
        "                pred_score = self.model(feature.unsqueeze(0)).squeeze(0)\n",
        "                pred_score = torch.softmax(pred_score, dim=0)[1]\n",
        "                video_info = data_file['video_'+str(idx)]\n",
        "                pred_score, pred_selected, pred_summary = select_keyshots(video_info, pred_score)\n",
        "                true_summary_arr = video_info['user_summary'][()]\n",
        "                eval_res = [eval_metrics(pred_summary, true_summary) for true_summary in true_summary_arr]\n",
        "                eval_res = np.mean(eval_res, axis=0).tolist()\n",
        "\n",
        "                eval_arr.append(eval_res)\n",
        "                table.add_row([idx] + eval_res)\n",
        "\n",
        "                out_dict[idx] = {\n",
        "                    'pred_score': pred_score, \n",
        "                    'pred_selected': pred_selected, 'pred_summary': pred_summary\n",
        "                    }\n",
        "        \n",
        "        score_save_path = self.config.score_dir + '/epoch-{}.json'.format(epoch_i)\n",
        "        with open(score_save_path, 'w') as f:\n",
        "            tqdm.write('Save score at {}'.format(str(score_save_path)))\n",
        "            json.dump(out_dict, f)\n",
        "        eval_mean = np.mean(eval_arr, axis=0).tolist()\n",
        "        table.add_row(['mean']+eval_mean)\n",
        "        tqdm.write(str(table))\n",
        "\n",
        "\n",
        "train_config = Config()\n",
        "test_config = Config(mode='test')\n",
        "train_loader, test_dataset = get_loader(train_config.data_path, batch_size=train_config.batch_size)\n",
        "solver = Solver(train_config, train_loader, test_dataset)\n",
        "losses=[]\n",
        "solver.train()\n",
        "print(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfQUGJtDf49s"
      },
      "source": [
        "losses_plt=[losses[i] for i in range(0,1600,10)]\n",
        "epochs=range(0,160)\n",
        "plt.xlabel('Epoch No.')\n",
        "plt.ylabel('Training Error')\n",
        "plt.plot(epochs,losses_plt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9_j7p6ggQmN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}